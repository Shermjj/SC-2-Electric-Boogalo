---
title: "Notes"
output:
  pdf_document: default
urlcolor: blue
date: "2024-05-15"
header-includes: 
  - \DeclareMathOperator*{\argmin}{arg\,min}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.width=12, fig.height=10)
```

```{r}
# devtools::install_github("mfasiolo/electBook")
library(electBook)
data(Irish)

```

# Gaussian Process Regression

Gaussian process regression seems like a good fit for our noisy data, especially since we will have a distribution of possible `demand` points for each `dateTime`, corresponding to the uncertainty in our predictions.

## Theory

A gaussian process is a collection of random variables, which have a joint Gaussian distribution. A Gaussian process is completely specified by its mean function and covariance function. We build the following model:

Let $y_i = f(x_i) + \varepsilon_i$, where $f(x) \sim \text{GP}(0, k(x, x'))$ and $\varepsilon_i \sim N(0, \sigma^2)$. Then we can find the posterior distribution of $f(x_*)$ given $y$ as:

$$
f(x_*) | y \sim N(\mu(x_*), \sigma^2(x_*))
$$

where $\mu(x_*) = k(x_*, x)^T(K + \sigma^2I)^{-1}y$ and $\sigma^2(x_*) = k(x_*, x_*) - k(x_*, x)^T(K + \sigma^2I)^{-1}k(x_*, x)$. In pratice, to find the posterior distribution, we maximise the marginal log-likelihood.

## Implementation

```{r}
#Plotting two different households agains eachother over a week
library(kernlab)
library(tidyverse)

set.seed(123)
#Seed of generic addresses to use
Id  <- sample(colnames(Irish$indCons), 1000,replace = FALSE)

#Clean data and ammend time dependent variables
#---------------------------------------------
irish_demand_sample <- Irish$indCons[,Id] %>%
  bind_cols(Irish$extra) %>% # add time-related variables
  pivot_longer(cols = all_of(Id), names_to = "Id", values_to = "demand") %>%
  # Data cleaning
  select(-time, -holy) %>% # Remove time and holy columns
  # Feature engineering
  mutate(
    hour = hour(dateTime), #Hour of day
    day = day(dateTime), #Day of week
    week = week(dateTime), #Week of Year
    month = month(dateTime), #Month of year
    weekend = ifelse(dow %in% c("Sat", "Sun"), 1, 0) #if weekend
  ) %>%
  mutate(temp_sq = temp^2) %>%  # quadratic term for temperature
  # One-hot encode the day of the week
  bind_cols(model.matrix(~ dow - 1, data = .)) %>%
  select(-dow)
#---------------------------------------------

#Split data into training and test sets
#---------------------------------------------
split_data <- function(data, month=11) {
  data_train <- data %>% 
    filter(month <= month)
  data_test <- data %>%
    filter(month == month)
  return(list(train = data_train, test = data_test))
}

irish_demand_test <- split_data(irish_demand_sample)$test
irish_demand_train <- split_data(irish_demand_sample)$train
#---------------------------------------------
#Plotting two different households agains eachother over a week
house1 <-  filter(irish_demand_sample,Id == Id[1] & week == 3)
house2 <-  filter(irish_demand_sample,Id == Id[2] & week == 3)

ggplot()+
  geom_point(data = house1, aes(x = dateTime, y = demand),color = "#000000")+
  geom_point(data = house2, aes(x = dateTime, y = demand),color = "#004550")+
  labs(title = "Electricity demand vs temperature for two sample households",
       x = "Temperature", y = "Electricity demand (kWh)")
```


```{r}


pool_data_by_catagory <- function(data, catagory = "week"){
  data %>% 
    group_by(!!sym(catagory)) %>% 
    summarise(mean_demand = mean(demand))
}

 
filter_by_date <- function(data,
                            day = c(1,7),
                            week = c(1,1),
                            month = c(1,1) ){
  data %>% 
    filter()
}

```
```{r}
library(kernlab)
library(dyplr)
library(caret)

# Define a function which performs Gaussian process regression
gaussian_process_reg <- function(data, kernel = "rbfdot",plot = FALSE) {
  train_indices <- createDataPartition(data$demand, p = 0.7, list = FALSE)
  train_set <- data[train_indices, ]

# Use the remaining 30% for the test set
  test_set <- data[-train_indices, ]

  x = as.vector(train_set$dateTime)
  y = as.vector(train_set$demand)
  # Define the Gaussian process model
  gpr_model <- kernlab::gausspr(x,y, kernel = kernel)
  prediction <- data.frame(
    dateTime = data$dateTime,
    mean = predict(gpr_model, as.vector(data$dateTime)))
  test_predictions <- predict(gpr_model,as.vector(test_set$dateTime))
  performance <- postResample(pred = test_predictions, obs = test_set$demand)

  data_predict <- left_join(data, prediction, by = "dateTime")
  if (plot){
    pl <- ggplot(data_predict, aes(x = dateTime)) +
      geom_point(aes(y = demand), color = "#000000") +
      geom_line(aes(y = mean), color = "red") +
      labs(title = "GPR model predictions",
           x = "Date", y = "Predicted demand")
    
  } else {
    pl <- NULL
  }
  return(list(model = gpr_model, data = data_predict, pl = pl,performance= performance))
}

#gaussian_process_reg(house1, plot = TRUE)$pl

```

```{r}
#gaussian_process_reg(house2, plot = TRUE)$pl
```

We can also perform gaussian process regression on aggregated data, in this case we will aggregate by the `class` column and average the demand:

```{r}
# Join the two dataframes on the 'ID' column
joined_df <- irish_demand_sample %>%
  rename(ID = Id) %>%
  inner_join(Irish$survey, by = "ID")

# Split the joined dataframe by the 'SOCIALCLASS' column
split_dfs <- split(joined_df, joined_df$SOCIALCLASS)

# Now, split_dfs is a list of dataframes, one for each social class
# You can access the dataframe for a specific social class like this:
df_class1 <- split_dfs[["AB"]]
df_class2 <- split_dfs[["C1"]]
df_class3 <- split_dfs[["C2"]]
df_class4 <- split_dfs[["DE"]]
df_class5 <- split_dfs[["F"]]

```



```{r}
#perform GPR on df_class1

df_class1week <- filter(df_class1,week <= 4 & week > 1 )
#head(df_class1week)

df_class1week_agg <- df_class1week %>%
  group_by(dateTime) %>%
  summarise(demand = sum(demand, na.rm = TRUE))

df_class1week_agg
gaussian_process_reg(df_class1week_agg,plot= TRUE)$pl
gaussian_process_reg(df_class1week_agg,plot= TRUE)$performance
```
